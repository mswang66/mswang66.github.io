<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[mariadb集群zookeeper管理逻辑]]></title>
    <url>%2F2018%2F02%2F02%2Fmariadb%E9%9B%86%E7%BE%A4zookeeper%E7%AE%A1%E7%90%86%E9%80%BB%E8%BE%91%2F</url>
    <content type="text"><![CDATA[启动之前逻辑 先获取锁 判断当前是否有服务在线 如果有服务在线，则直接 slave 形式启动，并在启动结束之后放弃锁 如果没有服务在线,3.1. 查看本地seq3.2. 如果seq &gt; 0 master形式 启动 放弃锁 3.3. 如果seq = -1 3.3.1 查看Zookeeper上是否存在last 3.3.2 如果不存在 直接启动 master形式 3.3.3 如果存在(可能是多个) 判断是否属于这2个 如果是，则master启动 如果不是， 则放弃锁，并重新获取申请锁 启动逻辑 run command 监控服务是否正常12345678910111213如果服务启动失败，那我们就撤，返回error，告诉kolla start如果正常启动， 首先第一件事情，向上注册一个临时节点(用作服务发现)，并且向上注册一个永久节点(用作重启是判断最后一个启动的服务) 然后监听其他两个节点的临时节点 临时节点监听逻辑： 获取锁(防止重启太快导致) 判断临时节点是否存在，如果存在，do nothing 如果不存在了，删除对应的永久节点 放弃锁 说明：可能多个客户端同时获取到删除逻辑，所以需要在获取到锁之后，再去判断一下该节点是否存在 突然挂了一、mysql挂了，容器没挂 删除本地对应的临时节点 (不用手动删除，zk close即可) 删除本地对应的永久节点这里需要判断当前节点是否是最后一个节点，如果是，则不删除本地的永久节点 二、整个容器挂了do nothing]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F02%2F02%2Fzookeeper%E5%9F%B9%E8%AE%AD%2F</url>
    <content type="text"><![CDATA[zookeeper培训培训内容如下 基本概念 zookeeper选举master流程 zookeeper的master和slave心跳方式 zookeeper如何保证数据一致性 zookeeper使用场景 如何使用zookeeper 1.基本概念1.1. Zookeeper特点： 最终一致性：为客户端展示同一个视图，这是zookeeper里面一个非常重要的功能 可靠性：如果消息被到一台服务器接受，那么它将被所有的服务器接受。 实时性：Zookeeper不能保证两个客户端能同时得到刚更新的数据，如果需要最新数据，应该在读数据之前调用sync()接口。 独立性 ：各个Client之间互不干预 原子性 ：更新只能成功或者失败，没有中间状态。 1.2. Zookeeper中的角色 领导者(Leader)：领导者负责进行投票的发起和决议，更新系统状态，处理写请求 跟随者(Follwer)：Follower用于接收客户端的读写请求并向客户端返回结果，在选主过程中参与投票 观察者（Observer）：观察者可以接收客户端的读写请求，并将写请求转发给Leader，但Observer节点不参与投票过程，只同步leader状态，Observer的目的是为了，扩展系统，提高读取速度。 在3.3.0版本之后，引入Observer角色的原因： 1234567Zookeeper需保证高可用和强一致性；为了支持更多的客户端，需要增加更多Server；Server增多，投票阶段延迟增大，影响性能；权衡伸缩性和高吞吐率，引入Observer ；Observer不参与投票；Observers接受客户端的连接，并将写请求转发给leader节点；加入更多Observer节点，提高伸缩性，同时不影响吞吐率。 客户端(Client)： 执行读写请求的发起方 1.3. 节点类型1234PERSISTENT：永久节点EPHEMERAL：临时节点PERSISTENT_SEQUENTIAL：永久节点、序列化EPHEMERAL_SEQUENTIAL：临时节点、序列化 说明： 临时节点是跟client绑定的，client断开连接，则临时节点会自动被清除 1.4. 回调方式1.4.1. WatcherWatcher是用于监听节点，session 状态的，比如getData对数据节点a设置了watcher，那么当a的数据内容发生改变时，客户端会收到NodeDataChanged通知，然后进行watcher的回调。 1.4.2. AsyncCallbackAsyncCallback是在以异步方式使用 ZooKeeper API 时，用于处理返回结果的。 例如：getData同步调用的版本是：byte[] getData(String path, boolean watch,Stat stat)，异步调用的版本是：void getData(String path,Watcher watcher,AsyncCallback.DataCallback cb,Object ctx)，可以看到，前者是直接返回获取的结果，后者是通过AsyncCallback回调处理结果的。 2.选举流程2.1. 选举流程如下1. 每个Server发出一个投票。由于是初始情况，Server1和Server2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和ZXID，使用(myid, ZXID)来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。 2. 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票、是否来自LOOKING状态的服务器。 3. 处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下 优先检查ZXID。ZXID比较大的服务器优先作为Leader。 如果ZXID相同，那么就比较myid。myid较大的服务器作为Leader服务器。 对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的ZXID，均为0，再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。 4. 统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。 5. 改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。 2.2. 流程源码解析选举算法主要在QuorumPeer类中的run方法中。主要的逻辑是选主这一块(也就是从looking状态开始)如下是代码中的大体流程 1.判断是否停止了选举 2.如果停止了选举，则新建一个选举算法 3.选举算法(默认算法就是快速选举也就是FastLeaderElection类)开始寻找master 4.群发notification，然后开始等待外接的消息， 收到消息如果是lookup则从5开始执行， 收到的消息如果是leading状态则从13开始 5.(5)判断收到的消息中的vote和本地的vote的选举轮次 6.如果本地的轮次高，则不做任何处理。如果本地的选举轮次低，则更新本地的选举轮次，并且重发上面的notification，因为上一次发送的数据不会被其他的节点处理 7.如果选举轮次一致，则判断节点的优先级，如果本地优先级低，则会吧本地选举的对象改为收到的vote(每个节点默认选举自己)，因为vote的内容改变了，所以会重发notification给其他节点 8.将新来的节点放到本地的一个选举的list中 9.判断最新的本地的vote在选举list中是否达到了一半以上（本地的vote不一定是自己） 10.如果达到了一半以上，先把本地还没处理的vote接收过来，跟当前的vote判断 11.如果后面处理的数据都没有当前的大(也就是n是等于null才推出while循环的)， 则更新到当前的状态，如果最大值是本地，则设置leading否则是learning(follower) 12.返回到QuorumPeer的循环中，然后就简单了，根据状态知道是启动leader还是follower。有一种可能是本地选举的跟别人不一致，则会在启动具体角色的时候连接超时。然后就会抛出异常，然后把本地的状态重新设置成lookup 13.(13)判断轮次是否一致，是的话，加入到本地的选举list。 14.判断是否有一半的vote是当前过来的leading，如果有，则设置状态完就直接返回 15.如果轮次不一致，加入到另一个专门用来存放leading的vote list(这个list主要用来处理新的节点加入到正常的状态中,已经结束选举的集群中节点在Messenger中，该类也在FastLeaderElection中) 16.判断这个list中的是否有一半以上的vote，有就直接确定状态，否则continue again 3.leader和follower如何保持心跳master发送ping包 详细可以见源码，在相应的leader和follower启动代码最后都有个while循环 4.zookeeper如何保证数据一致性 主要的原则是写只能由leader来发起，读可以在所有节点执行 客户端连接到集群中某一个节点 客户端发送写请求 服务端连接节点，把该写请求转发给leader leader处理写请求，一半以上的从节点也写成功，返回给客户端成功。 4.1. 还有需要注意的是zookeeper对数据大小有严格要求ZooKeeper是一套高吞吐量的系统，为了提高系统的读取速度，ZooKeeper不允许从文件中读取需要的数据，而是直接从内存中查找。 换句话说，ZooKeeper集群中每一台服务器都包含全量的数据，并且这些数据都会加载到内存中。同时ZNode的数据并支持Append操作，全部都是Replace。 所以从上面分析可以看出，如果ZNode的过大，那么读写某一个ZNode将造成不确定的延时;同时ZNode过大，将过快地耗尽ZooKeeper服务器的内存。这也是为什么ZooKeeper不适合存储大量的数据的原因。 5.zookeeper的使用场景5.1. Barrier Barrier屏障是分布式系统中常用的用于阻塞所有执行在某个执行点目的，直到某个条件被满足后再让所有的节点继续执行，ZooKeeper实现屏障的步骤是：12345 a. 创建/zk_barrier节点，当这个znode存在的时候表示barrier生效； b. 所有客户端调用exist()接口向/zk_barrier注册watch event； c. 如果exists()返回true则表示barrier生效，此时客户端阻塞等待notification； d. 负责/zk_barrier的节点检查直到某个条件满足后，删除/zk_barrier; e. 上面的删除操作会触发watch event，此时所有阻塞在此的客户端都接收到notification，他们再次调用exists()都返回false，表明barrier已经被消除，则所有客户端开始执行。 根据上面的原理，还可以轻松的实现double-barrier，可以用于控制计算的开始、退出时间点，原理是当指定数目的线程加入到barrier中时允许计算开始执行，而当所有的计算节点计算完成退出后，整个计算过程被标示为结束状态。其过程可以描述为： Phase 11234 a. 假设/barrier节点为目的点，每一个客户端处理进程都会在/barrier目录下创建ephemeral节点，而通常客户端会用自己的主机名标识自己这个临时节点；同时，客户端还需要设置/ready节点的watch event监听； b. N是预先知晓的客户端处理进程数目最小阈值，每当一个客户端进程加入之后，都调用M = getChildren(/barrier, watch=false)检查子节点的数目； c. 当M &gt;= N的时候，该加入的客户端处理进程负责创建/ready节点； d. 上面的创建操作会触发所有其他客户端开始计算操作； Phase 2123 a. 当客户端进程计算结束之后，都删除当时自己创建的临时节点； b. 然后客户端进程调用M = getChildren(/barrier, watch=true)，当M !=0 时候继续等待，而当M == 0时候，则大家就可以全部退出barrier了。 上面实现的不理想之处是存在惊群(herd effect)效应，改进的方式是创建临时节点时候采用sequential ephemeral节点，而在最后退出的时候，每个客户端进程只对临近于自己较小的顺序节点添加watch event即可。 5.2. Queue Queue作为联系生产者和消费最常用纽带，在分布式系统中也经常会被使用。通常而言生产者在某个节点下创建子节点以表示为“生产”行为，而消费者删除子节点表示“消费”过程，当然常常任务队列还需要FIFO的顺序特性，此时创建的子节点可以则可以是sequential顺序节点。1234 a. 创建/QUEUE表示任务队列； b. 生产者通过创建顺序子节点表示产生消息，其调用为create(“queue-“, SEQUENTIAL_EPHEMERAL)，其产生的消息形如queue-N； c. 消费者通过调用 M = getChildren(/QUEUE, watch=true)，该调用会返回一个子节点列表，通过排序字节点列表并从中选出序列号最小的子节点出来，消费者可以删除掉这个字节点表示已经消费之； d. 消费者一直等到M中的子节点消费完后，再调用一次getChildren()调用，以查看是否有新的子节点被添加进来。 上面的操作在删除子节点的时候可能会有问题，就是在其他客户端获取该自己点访问的时候，此时删除之会返回失败，客户端则需要重新尝试删除之(这么看来无法保证节点只被一个消费者消费啊)。queue实现的例子可以从recipes中查看官方样例。基于上面的例子，实现PriorityQueue也很简单，需要创建的队列名字是”queue-YY”即可，其中YY用于标示队列的优先级。 5.3. Lock分布式锁是分布式系统中用于同步访问共享资源的重要原语，分布式锁要求多个客户端不会同时持有该锁资源，以实现某一时刻只有一个客户端可以对指定的资源进行访问操作，比如写共享的文件、数据库。 为了创建分布式锁，首先需要创建一个持久节点作为锁节点，客户端如果需要这个锁就必须在其下创建临时序列ephemeral-sequential节点，然后我们约定拥有最小序列号的节点拥有该锁，而当客户端释放锁的时候，只需要删除自己这个临时节点就可以了，其请求锁过程可以表述如下：12345 a. 创建锁节点/locknode，客户端获取锁的时候创建顺序临时节点create(“/locknode/lock-“, CreateMode=EPHEMERAL_SEQUENTIAL)； b. 检查锁节点的所有子节点getChildren(“/locknode/lock-“, watch=false)，这里设置watch event为false是为了避免惊群效应； c. 如果在步骤a中创建的节点拥有最小序列号，则该节点获取到该锁，退出获取锁的算法过程； d. 否则就调用exists(“/locknode/znode具有相邻较小序列号, watch=true)，如果返回false则进入步骤b； e. 如果上面步骤返回true，则客户单安心等待释放锁的通知既可以； 释放锁的过程如下：12 a. 当持有锁的客户端直接删除相应的临时节点，这个删除操作也会让等待其释放所的其他某个客户端会收到notification； b. 那个收到通知的客户端应当在此时具有最小序列号的客户端，其得到通知的过程就应当是获取锁的过程。 5.4. Leader Election 选主就是要求分布式系统中只有一个客户端进程作为组织、协调者的情况，这样可以简化多个客户端进程的同步、分配、管理等工作，选主操作的重要作用就是消除Leader带来的单点问题，能够在Leader挂掉的情况下快速选出新Leader继续服务。通常，选主操作必须满足的条件是在任何时候，至多只有一个Leader存在。 在下面的选主操作中，会使用临时序列节点，我们同样假定具有最小序列号的临时节点代表的主机为Leader。最简单的方式就是当作为Leader最小序列号的节点消失后，所有节点收到通知并检查自己是否是最小节点，然后具有最小序列号的那个节点行驶Leader操作，这必然是惊群的实现。1234567 a. 创建/election持久节点作为所有待参选客户端根节点，参选的客户端创建临时序列节点/election/candidate-sessionID_路径，使用sessionID的识别码主要是用于帮助异常情况的名字识别，在创建临时顺序节点的时候可能create()已经成功但是server此时crash掉了，那么客户端就无法得到当前创建的名字，而此时client的会话仍然是有效的，此时客户端通过扫描sessionID可以得到相关节点，这里所涉及到的ZooKeeper的特性，是序列节点的序列号是基于目录递增的，而不是基于特定前缀递增的。下面假设create()调用成功后返回N序列号； b. 客户端可以获取当前参选者信息L = getChildren(“/election“, watch=false)，不设置watch event是为了防止竞选过程中出现惊群情况； c. 对/election/candidate-sessionID_M设置watch event，其中M是小于N相邻序列号L = getChildren(“/election/candidate-sessionID_M”, watch=true)； d. 当得到上面节点的删除notification的时候，调用getChildren(“/election/“, watch=false)获取所有的参选节点； e. 当得到最新的参选列表L时候，此时：如果本节点candidate-sessionID_N是L中的最小节点，则声明自己为Leader；否则像上面一样监听/election/candidate-sessionID_M，其中M是小于N相邻序列号； f. 如果当前的Leader已经crash掉，则拥有次最小序列号的客户端收到通知，通过上面的d步骤检查将会作为leader继任； 可选的，Leader可以将自己的识别信息保留在固定节点上，这时候其他节点将可以方便的查询那个固定节点的信息，得知当前系统的Leader是谁了。 5.5. Group membership 群组管理功能是允许其他进程能够相互感知进程加入、退出集群的功能，以便得到当前集群的最新状态。在ZooKeeper中通过ephemeral临时节点的特性，任何客户端加入集群都可以在一个预设的路径作为父节点创建一个ephemeral临时节点，而通过在这个父节点上增加watch event就可以感知加入、离开集群的操作导致集群成员的变化。1234 a. 创建/membership永久节点，任何客户端加入群组都需要在这个路径下创建ephemeral临时节点； b. 群组的所有成员注册/membership的watch event，通过调用L = getChildren(“/membership”, watch=true)方法，以感知群组成员的变化事件； c. 当有客户端加入群组的时候就会创建ephemeral临时节点，客户端离开或者crash等情况，ZooKeeper服务会自动删除其对应的临时节点，所有的其他成员将得到通知； d. 通过查看L，成员可以得知加入或者离开群组的成员； 当然，上面的实现会有herd effect惊群问题。 5.6. Two-Phase commit 2PC是事务系统中最常见的一致性手段，用于保证某个事务在所有客户端都是原子性commit或rollback。2PC的两个阶段是：协调者询问所有参与者，让所有参与者投票是提交还是放弃某个事务；协调者收集所有选票，如果所有参与者都赞成commit那么就提交事务，否则就回滚事务，协调者最终将结果通知给所有参与者。 123456 a. 创建/2PC_Transactions代表2PC事务节点，协调者(可以通过之前的选主方式选出Leader作为协调者)在其下创建事务，然后在其上设置watch event； b. 协调者在事务节点下创建另外一个持久子节点/2PC_Transactions/TX/tx_result，用于发布commit、abort或者其他的协议相关信息； c. 所有的事件参与者都在/2PC_Transactions/和/2PC_Transactions/TX/tx_result下创建watch event； d. 当协调者创建事务节点的时候，所有的参与客户端都会收到notification，然后客户端针对事务信息发起自己的投票；投票的过程就是在/2PC_Transactions/TX下创建带有自己主机标示的ephemeral临时节点，并表明自己的投票立场； e. 协调者发现应当参与的所有参与者都建立了临时节点后，就清点各个客户端的选票，根据投票结果将最终的事务结果写入到tx_result节点中； f. 当tx_result节点更新的时候，所有的客户端都会收到NodeDataChanged事件通知，然后根据最终的决议结果发起提交还是回滚操作。 5.7. Service Discovery 服务发现是分布式系统的核心功能和SOA架构的核心构件，最简单的服务发现是可以让客户端发现服务提供者的IP:Port地址信息。服务发现的核心特性包括：其允许服务进行注册以表明自己的可用性；其通过某种方式可以定位到一个现存可用的服务；服务的变更可以通知传播出去。123 a. 约定/services是一个固定节点用于服务发布； b. 服务注册发布的过程中，服务会在/services下面注册一个ephemeral临时节点； c. 服务发现的过程，客户端加入集群后可以对某个服务的路径添加watch侦听，新服务实例的加入和退出都可以被通知到。 6. 如何使用zookeeper预备1: 首先在开始之前我们先讲一下zookeeper的client的用法。请看如下代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051import org.apache.zookeeper.KeeperException;import org.apache.zookeeper.WatchedEvent;import org.apache.zookeeper.Watcher;import org.apache.zookeeper.ZooKeeper;import java.io.IOException;import java.util.concurrent.TimeUnit;//必须实现watcher，然后复写process方法public class TestExist implements Watcher&#123; private ZooKeeper zk; public static void main(String[] args) &#123; try &#123; //建一个watcher的对象 TestExist te = new TestExist(); //连接到zookeeper，需要注意最后一个参数watch ZooKeeper zk = new ZooKeeper(&quot;localhost:2181&quot;, 3000, te); te.zk = zk; zk.exists(&quot;/my&quot;,true); TimeUnit.SECONDS.sleep(1000000); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; &#125; //回调处理函数 @Override public void process(WatchedEvent event) &#123; System.out.println(&quot;=============Start&quot;); System.out.println(event.getPath()); System.out.println(event.getState()); System.out.println(event.getType()); System.out.println(&quot;=============End&quot;); System.out.println(&quot;\n&quot;); try &#123; //注意一点，watcher只能回调一次 zk.exists(&quot;/my&quot;,true); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 【注意】zookeeper所有的watcher只是监控一次，处理过后如有需要监控还需要再次调用exist或者其他相应的接口 预备2: 临时节点 现场演示 6.1. 分布式锁123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102package org.mariadbCtrl;import org.apache.zookeeper.*;import java.util.Collections;import java.util.Comparator;import java.util.List;import java.util.concurrent.TimeUnit;public class DistributeLock implements Watcher &#123; public ZooKeeper zk; private String root; private String path; private String sequence; private Object mutex; private List&lt;String&gt; seqs; public DistributeLock(ZooKeeper zk, String root, String path) &#123; mutex = new Object(); this.root = root; this.path = path; this.zk = zk; &#125; public void sortSequence(List&lt;String&gt; seqs) &#123; Collections.sort(seqs, new Comparator&lt;String&gt;() &#123; public int compare(String a, String b) &#123; return a.compareTo(b); &#125; &#125;); &#125; public void lock() &#123; try &#123; sequence = zk.create(root + &apos;/&apos; + path, new byte[0], ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL); sequence = sequence.split(&quot;/&quot;)[2]; System.out.println(&quot;Current Lock Sequence is &quot; + sequence); int cIndex = getNodeIndex(); System.out.println(&quot;First get index : &quot;+ cIndex); while (cIndex != 0) &#123; System.out.println(&quot;Now I listen to : &quot;+ root + &quot;/&quot; +seqs.get(cIndex - 1)); zk.exists(root + &quot;/&quot; + seqs.get(cIndex - 1), this); synchronized (mutex) &#123; mutex.wait(); cIndex = getNodeIndex(); System.out.println(&quot;Loop get index : &quot;+ cIndex ); &#125; &#125; System.out.println(&quot;I get the lock!Now I am going to sleep&quot;); TimeUnit.SECONDS.sleep(10000); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; public int getNodeIndex()&#123; int index = 0; try &#123; seqs = zk.getChildren(root, false); sortSequence(seqs); index = seqs.indexOf(sequence); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return index; &#125; public void unlock() &#123; try &#123; zk.delete(root + &quot;/&quot; + sequence, 0); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; catch (KeeperException e) &#123; e.printStackTrace(); &#125; &#125; @Override public void process(WatchedEvent event) &#123; if (event.getType() == Event.EventType.NodeDeleted) &#123; System.out.println(&quot;Watcher Event Node [&quot; + sequence + &quot;] has been removed&quot;); synchronized (mutex) &#123; mutex.notify(); &#125; &#125; else &#123; System.out.println(&quot;Event type is &quot; + event.getType()); System.out.println(&quot;State type is &quot; + event.getState()); &#125; &#125;&#125;]]></content>
  </entry>
  <entry>
    <title><![CDATA[mariadb集群重启过程]]></title>
    <url>%2F2018%2F01%2F23%2FMariaDB%20Galera%20Cluster%E9%87%8D%E5%90%AF%E8%BF%87%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[MariaDB Galera Cluster:Solution 1:1) I’ve changed safe_to_bootstrap parameter to 1 on one of the node in the file /var/lib/mysql/grastate.dat: safe_to_bootstrap: 1 2) After that I killed all mysql processes: killall -KILL mysql mysqld_safe mysqld mysql-systemd 3) And started a new cluster: galera_new_cluster 4) All other nodes I reconnected to the new one: systemctl restart mariadb P.S. to install killall on CentOS use psmisc: sudo yum install psmisc Solution 2:Another way to restart a MariaDB Galera Cluster is to use –wsrep-new-cluster parameter. 1) Kill all mysql processes: killall -KILL mysql mysqld_safe mysqld mysql-systemd 2) On the most up to date node start a new cluster: /etc/init.d/mysql start --wsrep-new-cluster 3) Now other nodes can be connected:12service mysql start --wsrep_cluster_address=&quot;gcomm://192.168.0.101,192.168.0.102,192.168.0.103&quot; \--wsrep_cluster_name=&quot;my_cluster&quot; Percona XtraDB Cluster:Solution 1:In case you can connect to the most up to date node then you can setup the node to bootstrap with the next SQL: SET GLOBAL wsrep_provider_options=&#39;pc.bootstrap=true&#39;; Solution 2:In case if all your nodes are dead and can not be started, you can stop the old one cluster and run a new one. You must stop all the cluster nodes because they have an information about old nodes in the old cluster. 1) Kill all the mysql processes on all nodes: killall -KILL mysql mysqld_safe mysqld mysql-systemd 2) Start a new cluster on the most up to date node: systemctl start mysql@bootstrap.service 3) Start other nodes: systemctl start mysql]]></content>
  </entry>
  <entry>
    <title><![CDATA[Openstack的vnc卡顿办法]]></title>
    <url>%2F2018%2F01%2F18%2Fopenstack%E7%9A%84vnc%E5%8D%A1%E9%A1%BF%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[openstack的vnc界面在chrome很卡的解决办法前言: 其实一直都发现这个问题，之前懒得去理，需要用的时候用vnc client就可以解决了。但是到客户那边需要演示，肯定需要用openstack界面去操作。这次修改也是因为现场演示的时候，chrome上的vnc直接卡死解决办法：1. 首先打开vnc的界面，通过console查看对应的js文件，来找出vnc的代码是哪个模块的代码2. 找过nova代码和horizon代码，发现都没有，这就超出我的预料了，我预想中应该在这2个项目中的3. 通过查看openstack的nova服务，发现 nova-novncproxy 服务存在，那肯定就在这个里面了，进去一开，果然4. 查看openstack的安装教程(安装教程)发现，是用yum安装的openstack-nova-novncproxy5. 然后发现这块代码其实是 noVNC项目的代码。openstack用的noVNC的版本是0.5.1。 google一查，说是0.6.2可以解决这个问题。6. 然后下载对应的 0.6.2 下载地址 版本直接替换，然后重启 openstack-nova-novncproxy服务，发现OK 到这里问题就解决了，但是我们不能修改openstack的yum源，所以，我们需要自己制作一个novnc的rpm包 规范的安装方式由于上述方法是手动替换noVNC的版本，这样太low了，不能作为安装的流程。So，我们需要做一个rpm包，然后替换原来的包。 解决方案1. 首先呢，得学习rpmbuild的安装教程，看了半天！突然想到，我把yum源的rpm包下载下来不就完事了嘛，只要简单修改一下2. 从openstack的yum源上找（openstack源Pike版本）,下载novnc-0.5.1-2.el7.noarch.rpm3. 然后解压缩 rpm2cpio novnc-0.5.1-2.el6.src.rpm |cpio -div, 解压出来会有3个文件 novnc-0.4-manpage.patch, novnc.spec， v0.5.1.tar.gz4. 然后修改nova.spec 文件。 只需要修改Version为 0.6.2,5. 然后开始build过程12345678910mkdir /root/rpmbuildcd /root/rpmbuildmkdir SOURCES SPECS BUILD RPMS SRPMScd -cp nonvc.spec /root/rpmbuild/SPECS/cp novnc-0.4-manpage.patch /root/rpmbuild/SOURCE/cp v0.6.2.tar.gz /root/rpmbuild/SOURCE/cd /root/rpmbuild/SPECS/rpmbuild -bb novnc.spec 当然了，需要自己准备rpmbuild的安装环境yum install gcc rpm-build pcre-devel 6. 这个时候你在/root/rpmbuild/RPMS/noarch目录下就会发现最新的 novnc-0.6.2-2.el7.noarch.rpm7. 编完之后就是安装了，由于之前openstack环境已经安装了 0.5.1 版本. So我们需要的是update1rpm -Uvh novnc-0.6.2-2.el7.noarch.rpm 到此结束]]></content>
  </entry>
  <entry>
    <title><![CDATA[cloudview修改密码]]></title>
    <url>%2F2018%2F01%2F16%2Fcloudview%E5%BA%94%E5%AF%B9%E4%BF%AE%E6%94%B9admin%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1. 修改配置文件1vi /etc/kolla/middleware/identity.json 替换password选项 2. 重启服务docker stop middleware docker start middleware]]></content>
  </entry>
  <entry>
    <title><![CDATA[kubernetes部署使用到的命令合集]]></title>
    <url>%2F2018%2F01%2F12%2Fkubernetes%E9%85%8D%E7%BD%AE%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[kubertest kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local [10.96.0.1 192.168.200.142] proxy_onexport no_proxy=20.0.0.0/24,20.0.0.1,20.0.0.130/24,kubertest,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster.local,localhost,127.0.0.0/8,10.0.0.0/24 no_proxy=20.0.0.0/24,20.0.0.133,20.0.0.130/24,k8server,kubernetes,kubernetes.default,kubernetes.default.svc,kubernetes.default.svc.cluster.local,localhost,127.0.0.0/8,10.0.0.0/24,k8server,10.0.0.134,10.0.0.2,10.0.0.10,200.0.0.0/24 swapoff -a sudo kubeadm init –pod-network-cidr=10.1.0.0/16 –service-cidr=10.3.3.0/24 kubeadm init –pod-network-cidr=20.0.0.0/24 –service-cidr=10.0.0.0/24 –kubernetes-version v1.9.1 –system-cgroups cgroupfs –cgroup-root / curl -L https://raw.githubusercontent.com/projectcalico/canal/master/k8s-install/1.6/canal.yaml -o canal.yamlsed -i “s@10.244.0.0/16@10.1.0.0/16@” canal.yamlkubectl taint nodes –all=true node-role.kubernetes.io/master:NoSchedule- /usr/bin/dockerd -H fd:// $DOCKER_OPTS /etc/docker/daemon.json{ “exec-opts”: [“native.cgroupdriver=systemd”] }kubeadm init \ –kubernetes-version=v1.8.0 \ –pod-network-cidr=20.0.0.0/24 \ –apiserver-advertise-address=192.168.200.142 Environment=”KUBELET_EXTRA_ARGS=–fail-swap-on=false” 关闭了局域网里面的dhcp服务 123IPADDR=20.0.0.133PREFIX=24BOOTPROTO=static yum install vim systemctl disable firewalldsystemctl stop firewalldsetenforce 0 sudo sed -i ‘s/10.96.0.10/10.0.0.134/g’ /etc/systemd/system/kubelet.service.d/10-kubeadm.conf kubeadm init –pod-network-cidr=20.0.0.0/24 –service-cidr=10.0.0.0/24 –bootstrap-auth-token certificate_manager.go:314] Failed while requesting a signed certificate from the master: cannot create certificate signing request: Postroot@k8server ~]# kubeadm init[init] Using Kubernetes version: v1.9.1[init] Using Authorization modes: [Node RBAC][preflight] Running pre-flight checks. [WARNING Hostname]: hostname “k8server” could not be reached [WARNING Hostname]: hostname “k8server” lookup k8server on 8.8.8.8:53: no such host [WARNING FileExisting-crictl]: crictl not found in system path[preflight] Starting the kubelet service[certificates] Generated ca certificate and key.[certificates] Generated apiserver certificate and key.[certificates] apiserver serving cert is signed for DNS names [k8server kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.0.134][certificates] Generated apiserver-kubelet-client certificate and key.[certificates] Generated sa key and public key.[certificates] Generated front-proxy-ca certificate and key.[certificates] Generated front-proxy-client certificate and key.[certificates] Valid certificates and keys now exist in “/etc/kubernetes/pki”[kubeconfig] Wrote KubeConfig file to disk: “admin.conf”[kubeconfig] Wrote KubeConfig file to disk: “kubelet.conf”[kubeconfig] Wrote KubeConfig file to disk: “controller-manager.conf”[kubeconfig] Wrote KubeConfig file to disk: “scheduler.conf”[controlplane] Wrote Static Pod manifest for component kube-apiserver to “/etc/kubernetes/manifests/kube-apiserver.yaml”[controlplane] Wrote Static Pod manifest for component kube-controller-manager to “/etc/kubernetes/manifests/kube-controller-manager.yaml”[controlplane] Wrote Static Pod manifest for component kube-scheduler to “/etc/kubernetes/manifests/kube-scheduler.yaml”[etcd] Wrote Static Pod manifest for a local etcd instance to “/etc/kubernetes/manifests/etcd.yaml”[init] Waiting for the kubelet to boot up the control plane as Static Pods from directory “/etc/kubernetes/manifests”.[init] This might take a minute or longer if the control plane images have to be pulled.[apiclient] All control plane components are healthy after 29.003333 seconds[uploadconfig] Storing the configuration used in ConfigMap “kubeadm-config” in the “kube-system” Namespace[markmaster] Will mark node k8server as master by adding a label and a taint[markmaster] Master k8server tainted and labelled with key/value: node-role.kubernetes.io/master=””[bootstraptoken] Using token: e3c0bf.09aed4130878a8ca[bootstraptoken] Configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials[bootstraptoken] Configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token[bootstraptoken] Configured RBAC rules to allow certificate rotation for all node client certificates in the cluster[bootstraptoken] Creating the “cluster-info” ConfigMap in the “kube-public” namespace[addons] Applied essential addon: kube-dns[addons] Applied essential addon: kube-proxy Your Kubernetes master has initialized successfully! To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config You should now deploy a pod network to the cluster.Run “kubectl apply -f [podnetwork].yaml” with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/ You can now join any number of machines by running the following on each nodeas root: kubeadm join –token e3c0bf.09aed4130878a8ca 10.0.0.134:6443 –discovery-token-ca-cert-hash sha256:53a22e0803f564192dca5de4898fdadc7d9b9734fd43262bfab7d137fc848d02 kubectl patch -f &lt;(cat &lt;&lt;EOFapiVersion: rbac.authorization.k8s.io/v1alpha1kind: ClusterRoleBindingmetadata: name: cluster-adminroleRef: apiGroup: rbac.authorization.k8s.io kind: ClusterRole name: cluster-adminsubjects: kind: Groupname: system:masters kind: Groupname: system:authenticated kind: Groupname: system:unauthenticatedEOF) 可能遇到的问题： [preflight] Some fatal errors occurred: /proc/sys/net/bridge/bridge-nf-call-iptables contents are not set to 1解决方案：echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-iptables echo 1 &gt; /proc/sys/net/bridge/bridge-nf-call-ip6tables]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04 安装 kubernetes]]></title>
    <url>%2F2018%2F01%2F11%2FUbuntu_16.04%E5%AE%89%E8%A3%85kubernetes%2F</url>
    <content type="text"><![CDATA[#Ubuntu 16.04 安装 kubernetes 1. 更新apt源123456# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key addOK# echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list# apt-get update 2. 安装docker手动安装方式： 打开 https://apt.dockerproject.org/repo/dists/ubuntu-xenial/main/filelist 可以看到有docker的版本列表下载需要的版本自行安装12345678910111213141516171819202122/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.1~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.3-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.0~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.0-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.05.0~ce-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.5-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.1-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.11.0-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.11.1-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.0-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.2-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.4-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.05.0~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.6-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.0~ce-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.1-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.11.2-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.0-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.04.0~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.1-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.1~ce-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.04.0~ce-0~ubuntu-xenial_armhf.deb 3. 安装kubernates基础组件1apt-get install -y kubelet kubeadm kubectl kubernetes-cni]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04 安装 kubernetes]]></title>
    <url>%2F2018%2F01%2F11%2FUbuntu%2016.04%20%E5%AE%89%E8%A3%85%20kubernetes%2F</url>
    <content type="text"><![CDATA[#Ubuntu 16.04 安装 kubernetes1. 更新apt源123456# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key addOK# echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list# apt-get update 2. 安装docker手动安装方式： 打开 https://apt.dockerproject.org/repo/dists/ubuntu-xenial/main/filelist 可以看到有docker的版本列表下载需要的版本自行安装12345678910111213141516171819202122/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.1~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.3-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.0~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.0-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.05.0~ce-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.5-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.1-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.11.0-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.11.1-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.0-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.2-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.4-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.05.0~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.6-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.0~ce-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.1-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.11.2-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.0-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.04.0~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.1-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.1~ce-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.04.0~ce-0~ubuntu-xenial_armhf.deb]]></content>
  </entry>
  <entry>
    <title><![CDATA[kubernetes环境搭建之minikube方式]]></title>
    <url>%2F2018%2F01%2F10%2Fkubernetes%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E4%B9%8Bminikube%2F</url>
    <content type="text"><![CDATA[minikube安装官网安装教程 官网已经有详细的教程，下面简要介绍一下安装步骤以及注意事项 安装步骤：1. 安装虚拟化产品 (kvm、virtualbox、vmware等等)2. 下载安装 kubectl3. 下载安装minikube注意事项1. 因为是google的玩意，所以只能翻出去。请自行搞个代理2. 第一次运行的时候一定要注意，需要根据自身虚拟化功能来设置比如： `minikube start --vm-driver=kvm2` 因为第一次运行的时候是要从官网下载对应的镜像。如果需要重新指定driver，最好把之前的iso删掉(我是把整个~/.minikube 都删掉了) 3. 第一次运行 minikube start --vm-driver=kvm2报错如下Error starting host: Error creating host: Error creating machine: Error in driver during machine creation: Error creating VM: virError(Code=55, Domain=19, Message=&apos;所需操作无效：network &apos;default&apos; is not active&apos;) 解决办法：需要去修改 libvirt的default网络详细步骤： 1、 virsh net-edit default 2、 修改ip address（我本地是因为ip和其他网卡冲突了） 3、 virsh net-start default (确保本地没有default 文件中的 bridge name)]]></content>
  </entry>
  <entry>
    <title><![CDATA[mac部署kubernetes]]></title>
    <url>%2F2018%2F01%2F04%2Fkubernetes%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA(MAC)%2F</url>
    <content type="text"><![CDATA[官方文档: https://kubernetes.io/docs/tasks/tools/install-minikube/ 1. Install kubectl下载kubectl1curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl 添加权限1chmod +x ./kubectl 2. 安装 minikube1curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ 3. 下载minikube iso1wget https://storage.googleapis.com/minikube/iso/minikube-v0.23.6.iso]]></content>
  </entry>
  <entry>
    <title><![CDATA[Onos cord 相关概念]]></title>
    <url>%2F2018%2F01%2F03%2FONOS-cord-%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[学习onos cord的知识储备 Part1.1. OLT Optional Line Terminal 光线路终端 2. ONF Optional Network Terminal 光网络终端 3. ONU Optional Network Unit 光网络单元 关系:局端设备（OLT）与多个用户端设备（ONU/ONT）之间通过无源的光缆、光分/合路器等组成的光分配网（ODN）连接的网络 区别:ONU和ONT都是用户端的设备，本质上没什么区别，但如果非得区分则可以从名称看出区别：ONT是光网络终端，应用于最终用户，而ONU是指光网络单元，它与最终用户之间可能还有其他网络 举例:比如在一个小区里面，ONT是直接放在用户家中的设备，而ONU可能就是放置在楼道中，各个用户通过交换机等设备连接至ONU。]]></content>
  </entry>
  <entry>
    <title><![CDATA[zookeeper之client连接]]></title>
    <url>%2F2017%2F12%2F26%2Fzookeeper%E4%B9%8Bclient%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[路径相关文件1. 目录： org.apache.zookeeper2. 文件列表 ClientCnxn.java ClientCnxnSocket.java ClientCnxnSocketNIO.java ClientCnxnSocketNetty.java ClientWatchManager.java Zookeeper.java 建立连接1. ZooKeeper.javaMethod 名: Zookeeper() Method 功能： 初始化 watch manager 设置默认 watch manager 解析 connect str 提供集群主机列表 初始化 ClientCnxnSocket对象(默认是 ClientCnxnSocketNIO) 创建ClientCnxn对象(主要操作都是靠这个对象来操作) start ClientCnxn对象 2. ClientCnxn.java2.1.1. Method 名：ClientCnxn()2.1.2. Method 功能： 设置对象的属性 new一个SendThread对象 new一个EventThread对象 2.2.1. Method 名: run()2.2.2. Method 功能：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要是开启一个while循环，用来接收server端发来的数据以及发送数据给server端 设置clientCnxnSocket里面的一些queue等参数 如果client状态是正常的，则开始while循环 [while] 判断clientCnxnSocket连接是否OK。如果no，则建立连接 [while]判断是否需要发送ping来保持连接 [while]如果当前是只读模式，则去寻找可读写的Server [while]执行clientCnxnSocket的doTransport方法 3. ClientCnxnSocketNIO3.1.1. Method 名： doTransport()3.1.2. Method 功能：主要调用 doIO 方法来执行接收发送操作 接收数据，读packets到incomingBuffer 发送数据. 发送/接收数据 关键函数：doIO 发送数据getChildren/exist/getData -&gt; cnxn.submitRequest -&gt; queuePacket -&gt; outgoingQueue.add - - - -&gt; doTransport -&gt; doIO -&gt; findSendablePacket -&gt; pendingQueue -&gt; pendingQueue.add()备注：发送数据会有while循环判断packet是否finish所以发送数据还需要参考接收数据的处理返回值 接收数据1. 处理callback和watcherdoTransport -&gt; doIO -&gt; sendThread.readResponse -&gt; eventThread.queue[Packets/Events] -&gt; Watcher.process /Callback.processResult 2. 处理返回值doTransport -&gt; doIO -&gt; sendThread.readResponse -&gt; ‘set packet.finish to true’ -&gt; 发送等待处返回结果]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F12%2F21%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
