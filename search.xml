<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Openstack的vnc卡顿办法]]></title>
    <url>%2F2018%2F01%2F18%2Fopenstack%E7%9A%84vnc%E5%8D%A1%E9%A1%BF%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[openstack的vnc界面在chrome很卡的解决办法前言: 其实一直都发现这个问题，之前懒得去理，需要用的时候用vnc client就可以解决了。但是到客户那边需要演示，肯定需要用openstack界面去操作。这次修改也是因为现场演示的时候，chrome上的vnc直接卡死解决办法：1. 首先打开vnc的界面，通过console查看对应的js文件，来找出vnc的代码是哪个模块的代码2. 找过nova代码和horizon代码，发现都没有，这就超出我的预料了，我预想中应该在这2个项目中的3. 通过查看openstack的nova服务，发现 nova-novncproxy 服务存在，那肯定就在这个里面了，进去一开，果然4. 查看openstack的安装教程(安装教程)发现，是用yum安装的openstack-nova-novncproxy5. 然后发现这块代码其实是 noVNC项目的代码。openstack用的noVNC的版本是0.5.1。 google一查，说是0.6.2可以解决这个问题。6. 然后下载对应的 0.6.2 下载地址 版本直接替换，然后重启 openstack-nova-novncproxy服务，发现OK 到这里问题就解决了，但是我们不能修改openstack的yum源，所以，我们需要自己制作一个novnc的rpm包 规范的安装方式由于上述方法是手动替换noVNC的版本，这样太low了，不能作为安装的流程。So，我们需要做一个rpm包，然后替换原来的包。 解决方案1. 首先呢，得学习rpmbuild的安装教程，看了半天！突然想到，我把yum源的rpm包下载下来不就完事了嘛，只要简单修改一下2. 从openstack的yum源上找（http://mirror.centos.org/centos-7/7/cloud/x86_64/openstack-ocata/common/）,下载novnc-0.5.1-2.el7.noarch.rpm3. 然后解压缩 rpm2cpio novnc-0.5.1-2.el6.src.rpm |cpio -div, 解压出来会有3个文件 novnc-0.4-manpage.patch, novnc.spec， v0.5.1.tar.gz4. 然后修改nova.spec 文件。 只需要修改Version为 0.6.2,5. 然后开始build过程12345678910mkdir /root/rpmbuildcd /root/rpmbuildmkdir SOURCES SPECS BUILD RPMS SRPMScd -cp nonvc.spec /root/rpmbuild/SPECS/cp novnc-0.4-manpage.patch /root/rpmbuild/SOURCE/cp v0.6.2.tar.gz /root/rpmbuild/SOURCE/cd /root/rpmbuild/SPECS/rpmbuild -bb novnc.spec 当然了，需要自己准备rpmbuild的安装环境yum install gcc rpm-build pcre-devel 6. 这个时候你在/root/rpmbuild/RPMS/noarch目录下就会发现最新的 novnc-0.6.2-2.el7.noarch.rpm7. 编完之后就是安装了，由于之前openstack环境已经安装了 0.5.1 版本. So我们需要的是update1rpm -Uvh novnc-0.6.2-2.el7.noarch.rpm 到此结束]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F01%2F16%2Fcloudview%E5%BA%94%E5%AF%B9%E4%BF%AE%E6%94%B9admin%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[1. 修改配置文件1vi /etc/kolla/middleware/identity.json 替换password选项 2. 重启服务docker stop middleware docker start middleware]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ubuntu 16.04 安装 kubernetes]]></title>
    <url>%2F2018%2F01%2F11%2FUbuntu_16.04%E5%AE%89%E8%A3%85kubernetes%2F</url>
    <content type="text"><![CDATA[#Ubuntu 16.04 安装 kubernetes 1. 更新apt源123456# curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key addOK# echo &quot;deb http://apt.kubernetes.io/ kubernetes-xenial main&quot; &gt; /etc/apt/sources.list.d/kubernetes.list# apt-get update 2. 安装docker手动安装方式： 打开 https://apt.dockerproject.org/repo/dists/ubuntu-xenial/main/filelist 可以看到有docker的版本列表下载需要的版本自行安装12345678910111213141516171819202122/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.1~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.3-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.0~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.0-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.05.0~ce-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.5-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.1-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.11.0-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.11.1-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.0-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.2-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.4-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.05.0~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.6-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.0~ce-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.1-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.11.2-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.13.0-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.04.0~ce-0~ubuntu-xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_1.12.1-0~xenial_amd64.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.03.1~ce-0~ubuntu-xenial_armhf.deb/volumes/repos/apt/repo/pool/main/d/docker-engine/docker-engine_17.04.0~ce-0~ubuntu-xenial_armhf.deb 3. 安装kubernates基础组件1apt-get install -y kubelet kubeadm kubectl kubernetes-cni]]></content>
  </entry>
  <entry>
    <title><![CDATA[kubernetes环境搭建之minikube方式]]></title>
    <url>%2F2018%2F01%2F10%2Fkubernetes%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E4%B9%8Bminikube%2F</url>
    <content type="text"><![CDATA[minikube安装官网安装教程 官网已经有详细的教程，下面简要介绍一下安装步骤以及注意事项 安装步骤：1. 安装虚拟化产品 (kvm、virtualbox、vmware等等)2. 下载安装 kubectl3. 下载安装minikube注意事项1. 因为是google的玩意，所以只能翻出去。请自行搞个代理2. 第一次运行的时候一定要注意，需要根据自身虚拟化功能来设置比如： `minikube start --vm-driver=kvm2` 因为第一次运行的时候是要从官网下载对应的镜像。如果需要重新指定driver，最好把之前的iso删掉(我是把整个~/.minikube 都删掉了) 3. 第一次运行 minikube start --vm-driver=kvm2报错如下Error starting host: Error creating host: Error creating machine: Error in driver during machine creation: Error creating VM: virError(Code=55, Domain=19, Message=&apos;所需操作无效：network &apos;default&apos; is not active&apos;) 解决办法：需要去修改 libvirt的default网络详细步骤： 1、 virsh net-edit default 2、 修改ip address（我本地是因为ip和其他网卡冲突了） 3、 virsh net-start default (确保本地没有default 文件中的 bridge name)]]></content>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F01%2F04%2Fkubernetes%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA(MAC)%2F</url>
    <content type="text"><![CDATA[官方文档: https://kubernetes.io/docs/tasks/tools/install-minikube/ 1. Install kubectl下载kubectl1curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/darwin/amd64/kubectl 添加权限1chmod +x ./kubectl 2. 安装 minikube1curl -Lo minikube https://storage.googleapis.com/minikube/releases/v0.24.1/minikube-darwin-amd64 &amp;&amp; chmod +x minikube &amp;&amp; sudo mv minikube /usr/local/bin/ 3. 下载minikube iso1wget https://storage.googleapis.com/minikube/iso/minikube-v0.23.6.iso]]></content>
  </entry>
  <entry>
    <title><![CDATA[Onos cord 相关概念]]></title>
    <url>%2F2018%2F01%2F03%2FONOS-cord-%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[学习onos cord的知识储备 Part1.1. OLT Optional Line Terminal 光线路终端 2. ONF Optional Network Terminal 光网络终端 3. ONU Optional Network Unit 光网络单元 关系:局端设备（OLT）与多个用户端设备（ONU/ONT）之间通过无源的光缆、光分/合路器等组成的光分配网（ODN）连接的网络 区别:ONU和ONT都是用户端的设备，本质上没什么区别，但如果非得区分则可以从名称看出区别：ONT是光网络终端，应用于最终用户，而ONU是指光网络单元，它与最终用户之间可能还有其他网络 举例:比如在一个小区里面，ONT是直接放在用户家中的设备，而ONU可能就是放置在楼道中，各个用户通过交换机等设备连接至ONU。]]></content>
  </entry>
  <entry>
    <title><![CDATA[zookeeper之client连接]]></title>
    <url>%2F2017%2F12%2F26%2Fzookeeper%E4%B9%8Bclient%E8%BF%9E%E6%8E%A5%2F</url>
    <content type="text"><![CDATA[路径相关文件1. 目录： org.apache.zookeeper2. 文件列表 ClientCnxn.java ClientCnxnSocket.java ClientCnxnSocketNIO.java ClientCnxnSocketNetty.java ClientWatchManager.java Zookeeper.java 建立连接1. ZooKeeper.javaMethod 名: Zookeeper() Method 功能： 初始化 watch manager 设置默认 watch manager 解析 connect str 提供集群主机列表 初始化 ClientCnxnSocket对象(默认是 ClientCnxnSocketNIO) 创建ClientCnxn对象(主要操作都是靠这个对象来操作) start ClientCnxn对象 2. ClientCnxn.java2.1.1. Method 名：ClientCnxn()2.1.2. Method 功能： 设置对象的属性 new一个SendThread对象 new一个EventThread对象 2.2.1. Method 名: run()2.2.2. Method 功能：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;主要是开启一个while循环，用来接收server端发来的数据以及发送数据给server端 设置clientCnxnSocket里面的一些queue等参数 如果client状态是正常的，则开始while循环 [while] 判断clientCnxnSocket连接是否OK。如果no，则建立连接 [while]判断是否需要发送ping来保持连接 [while]如果当前是只读模式，则去寻找可读写的Server [while]执行clientCnxnSocket的doTransport方法 3. ClientCnxnSocketNIO3.1.1. Method 名： doTransport()3.1.2. Method 功能：主要调用 doIO 方法来执行接收发送操作 接收数据，读packets到incomingBuffer 发送数据. 发送/接收数据 关键函数：doIO 发送数据getChildren/exist/getData -&gt; cnxn.submitRequest -&gt; queuePacket -&gt; outgoingQueue.add - - - -&gt; doTransport -&gt; doIO -&gt; findSendablePacket -&gt; pendingQueue -&gt; pendingQueue.add()备注：发送数据会有while循环判断packet是否finish所以发送数据还需要参考接收数据的处理返回值 接收数据1. 处理callback和watcherdoTransport -&gt; doIO -&gt; sendThread.readResponse -&gt; eventThread.queue[Packets/Events] -&gt; Watcher.process /Callback.processResult 2. 处理返回值doTransport -&gt; doIO -&gt; sendThread.readResponse -&gt; ‘set packet.finish to true’ -&gt; 发送等待处返回结果]]></content>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2017%2F12%2F21%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
